# -*- coding: utf-8 -*-
"""Selected_3proect_(2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YikmF9OjlMkbWkBfBox6gn9fxvSKI5pB
"""

from google.colab import drive
drive.mount('/content/drive')

# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import re
import string
from nltk.corpus import stopwords
from nltk.stem.isri import ISRIStemmer
from collections import Counter
import numpy as np
import csv
import pandas 
from itertools import zip_longest
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

reader = "/content/drive/MyDrive/next_page_links - Sheet1 (1).csv"
path = '/content/drive/MyDrive'

bookNames =[]
bookCats = []
bookLinks = []
authorNames = []

result = requests.get("https://www.arab-books.com/")
src = result.content
soup = BeautifulSoup(src,"lxml")
book_links = soup.find_all("h3",{"class": "post-title"})
book_links = str(book_links)
x = re.findall('(?:(?:https?|ftp):\/\/)?[\w/\-?=%.]+\.[\w/\-&?=%.]+', book_links)
author = soup.find_all("div",{"class": "book-writer"})
#print(author)
#print(type(x))
print(x) 
book = requests.get(x[0])
book_content = book.content
soup_ = BeautifulSoup(book_content,"lxml")
book_cat = soup_.find_all("span" , {"class": "post-cat-wrap"})
book_name = soup_.find_all("h1" , {"class": "post-title entry-title"})
link = soup_.find_all("div" , {"class": "down-link-bottom"})
link = str(link)
pdf_link = re.findall('(?:(?:https?|ftp):\/\/)?[\w/\-?=%.]+\.[\w/\-&?=%.]+', link)
for i in range(len(book_name)):
  bookNames.append(book_name[i].text)
  bookCats.append(book_cat[i].text)
  authorNames.append(author[i].text)
  bookLinks.append(pdf_link[i])
print(bookNames,bookCats,authorNames,bookLinks)

links = pandas.read_csv(reader)
#links.head()

next_page_link = links['page links'].values
print(next_page_link)

for i in range(0,len(next_page_link)):
    result = requests.get(next_page_link[i],allow_redirects=False)
    src = result.content
    soup = BeautifulSoup(src,"lxml")
    book_links = soup.find_all("h3",{"class": "post-title"})
    book_links = str(book_links)
    x = re.findall('(?:(?:https?|ftp):\/\/)?[\w/\-?=%.]+\.[\w/\-&?=%.]+', book_links)
    author = soup.find_all("div",{"class": "book-writer"})
    #print(type(x))
    #print(x)
    for j in range(0,len(x)):
        #print(x)
      book = requests.get(x[j],allow_redirects=False)
      book_content = book.content
      soup_ = BeautifulSoup(book_content,"lxml")
      book_cat = soup_.find_all("span" , {"class": "post-cat-wrap"})
      book_name = soup_.find_all("h1" , {"class": "post-title entry-title"})
      link = soup_.find_all("div" , {"class": "down-link-bottom"})
      link = str(link)
      pdf_link = re.findall('(?:(?:https?|ftp):\/\/)?[\w/\-?=%.]+\.[\w/\-&?=%.]+', link)
      #print(book_cat,book_name,pdf_link)
      for m in range(len(pdf_link)):
        bookLinks.append(pdf_link[m])
      for n in  range(len(book_name)):
        bookNames.append(book_name[n].text)
        bookCats.append(book_cat[n].text)
        authorNames.append(author[n].text)
    print(bookNames,bookCats,authorNames,bookLinks)

file_list = [bookNames,bookCats,authorNames,bookLinks]
exported = zip_longest(*file_list)
with open("/content/drive/MyDrive/Selected_3/Scrapped_Data.csv", "w") as myfile: 

	wr = csv.writer(myfile)
	wr.writerow(['اسم الكاتب', 'عنوان الكتاب', 'نوع الكتاب', 'رابط الكتاب'])
	wr.writerows(exported)

header = ['اسم الكاتب', 'عنوان الكتاب', 'نوع الكتاب', 'رابط الكتاب']
extracted_data_file     = '/content/drive/MyDrive/scrping_data.csv'
cleaned_data_file       = '/content/drive/MyDrive/data_modified.csv'
autocorrected_data_file = '/content/drive/MyDrive/data_autocorrected.csv'
dictionary_file         = './arabic-wordlist-1.6.txt'
delimiter = '،،،،'

from google.colab import drive
drive.mount('/content/drive')

data_path = "/content/drive/MyDrive/scrping_data.csv"
path = '/content/drive/MyDrive'
# ---------------------------------------------------------------------------- #


### Step 2) Data_preprocessing

class preprocessing(object):
    
    def __init__(self, authors, titles, categories, links):
        
        modified_authors    = self.Data_preprocessing(authors)
        modified_titles     = self.Data_preprocessing(titles)
        modified_categories = self.Data_preprocessing(categories)
        ## Write Cleaned Scrapped Data Into New Text File
        #save modified Data In csv File 
        with open(cleaned_data_file, "w", encoding='utf-8-sig') as file:
            wr = csv.writer(file)
            wr.writerow(header)
            colum1 = modified_authors.split(delimiter)
            colum2 = modified_titles.split(delimiter)
            colum3 = modified_categories.split(delimiter)
            
            for (l1, l2, l3, l4) in zip(colum1, colum2, colum3, links):
                s = u','.join([str(l1), str(l2), str(l3), str(l4)]) + u'\n'
                file.write(s)
                
          
      

    def Data_preprocessing(self, text):
        ## 1.1 Replace punctuations with a white space
        remove_punctuations = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)
        ## 1.2 Normalize
        normalized = re.sub("گ", "ك", remove_punctuations)
        normalized = re.sub("ى", "ي", normalized)
        ## 2.1 Remove Non-Arabic Words
        remove_nonarabic = re.sub(r'\s*[A-Za-z]+\b', ' ' , normalized)
        ## 2.2 Remove Stop Words.
     # stop_words = set(stopwords.words('arabic'))
      # filtered_sentence = ' '.join([word for word in remove_nonarabic.split() 
                                           #if word not in stop_words])
        ## 3.0 Stemming
        st = ISRIStemmer()
        st.stem(remove_nonarabic)
        
        return remove_nonarabic
    
# ---------------------------------------------------------------------------- #

class SpellChecker(object):

    def read_corpus(self, filename):
        with open(data_path, "r", encoding='utf-8-sig') as file:
            lines = file.readlines() ## Read The File Line By Line
            words = []
            for line in lines:
              words += re.findall(r'\w+', line.lower()) ## Put Each Word In Its Lowercase.
    
        return words
    
    def read_corpus_lines(self, filename):
        lines1 = []
        lines2 = []
        lines3 = []
        lines4 = []
        with open(filename, "r", encoding='utf-8-sig') as csvfile:
            csvreader = csv.DictReader(csvfile)
            for row in csvreader:
                lines1.append(row["اسم الكاتب"])
                lines2.append(row["عنوان الكتاب"])
                lines3.append(row["نوع الكتاب"])
                lines4.append(row["رابط الكتاب"])
        return lines1, lines2, lines3, lines4
    
    def get_count(self, word_list):
        word_count_dict = {}  ## Each Word Count
        word_count_dict = Counter(word_list)
        return word_count_dict
    
    def get_probs(self, word_count_dict):
        ## 𝑃(𝑤ᵢ) = 𝐶(𝑤ᵢ) / M 
        m = sum(word_count_dict.values())
        word_probs = {w: word_count_dict[w] / m for w in word_count_dict.keys()}
        
        return word_probs
    
    def _split(self, word):
        return [(word[:i], word[i:]) for i in range(len(word) + 1)]
    
    def _delete(self, word):
        return [l + r[1:] for l,r in self._split(word) if r]
    
    def _swap(self, word):
        return [l + r[1] + r[0] + r[2:] for l, r in self._split(word) if len(r)>1]
    
    def _replace(self, word):
        letters = string.ascii_lowercase
        return [l + c + r[1:] for l, r in self._split(word) if r for c in letters]
    
    def _insert(self, word):
        letters = string.ascii_lowercase
        return [l + c + r for l, r in self._split(word) for c in letters]
       
    def _edit1(self, word):  
        return set(self._delete(word) + self._swap(word) + 
                   self._replace(word) + self._insert(word))
    
    def _edit2(self, word):
      return set(e2 for e1 in self._edit1(word) for e2 in self._edit1(e1))
    
    def correct_spelling(self, word, vocabulary, word_probability):
        if word in vocabulary:
            #print(f"\n'{word}' is already correctly spelt")
            return 
        
        suggestions = self._edit1(word) or self._edit2(word) or [word]
        best_guesses = [w for w in suggestions if w in vocabulary]
          
        return [(w, word_probability[w]) for w in best_guesses]
            
    
    def correct_word(self, word, corrections):
        if corrections:
            print('\nSuggested Words:', corrections)
            probs = np.array([c[1] for c in corrections])
            ## Get The Index Of The Best Suggested Word (Higher Probability)
            best_ix = np.argmax(probs) 
            correct = corrections[best_ix][0]
            print(f"\n'{correct}' is Suggested for '{word}'")
            return correct        
        
# ---------------------------------------------------------------------------- #



### preprocessing
def split_file(filename):
    data = pandas.read_csv(data_path)
    
    # converting columns data to list
    authors = data['اسم الكاتب'].tolist()
    titles = data['عنوان الكتاب'].tolist()
    categories = data['نوع الكتاب'].tolist()
    links = data['رابط الكتاب'].tolist()

    return authors, titles, categories, links

authors, titles, categories, links = split_file(extracted_data_file)

authors = list(map(lambda s: s+delimiter, authors))
titles = list(map(lambda s: s+delimiter, titles))
categories = list(map(lambda s: s+delimiter, categories))

preprocessing(str(authors), str(titles), str(categories), links)        

### AutoCorrection
def autocorrect_misspellings(lines, vocabs, word_prob):
    correct_lines = []
    for line in lines:
        for word in line.split(' '):
            corrections = spell_checker.correct_spelling(word, vocabs, word_prob)
            correct     = spell_checker.correct_word(word, corrections)
            if correct:
                correct_lines.append(correct)
            else:
                correct_lines.append(word)

    correct_lines = ' '.join(correct_lines)
    
    return correct_lines

spell_checker = SpellChecker()
print("Starting AutoCorrecting Misspelling....\n")
base_words = spell_checker.read_corpus(dictionary_file)
vocabs = set(base_words) ## Vocabulary (Unique Words)

word_dict_counts = spell_checker.get_count(base_words)
word_prob   = spell_checker.get_probs(word_dict_counts) 

authors, titles, categories, links = spell_checker.read_corpus_lines(cleaned_data_file)   
authors = list(map(lambda orig_string: orig_string+'،،،،', authors))   
titles = list(map(lambda orig_string: orig_string+'،،،،', titles))   
categories = list(map(lambda orig_string: orig_string+'،،،،', categories))   

correct_authors    = autocorrect_misspellings(authors, vocabs, word_prob)
correct_titles     = autocorrect_misspellings(titles, vocabs, word_prob)
correct_categories = autocorrect_misspellings(categories, vocabs, word_prob)

with open(autocorrected_data_file, "w", encoding='utf-8-sig') as file:
    print("-- SAVING Autocorrected Data In csv File --")
    wr = csv.writer(file)
    wr.writerow(header)
    
    lines1 = correct_authors.split(delimiter)
    lines2 = correct_titles.split(delimiter)
    lines3 = correct_categories.split(delimiter)
    
    for (l1, l2, l3, l4) in zip(lines1, lines2, lines3, links):
        s = u','.join([str(l1), str(l2), str(l3), str(l4)]) + u'\n'
        file.write(s)
        
print("AutoCorrection Finished!\n")

cleaned_data_file = pandas.read_csv('/content/drive/MyDrive/S3/cleaned_data.csv')

cleaned_data_file = make_blobs(n_samples=6000,
                     centers=5,
                     n_features=3,
                     cluster_std=1.6,
                     random_state=100)

points = cleaned_data_file[0]

kmeans = KMeans(n_clusters=5)

kmeans.fit(points)

plt.scatter(cleaned_data_file[0][:,0],cleaned_data_file[0][:,1])

clusters = kmeans.cluster_centers_

print(clusters)

y_km = kmeans.fit_predict(points)

plt.scatter(points[y_km == 0,0], points[y_km == 0,1], s=50, color='red')
plt.scatter(points[y_km == 1,0], points[y_km == 1,1], s=50, color='green')
plt.scatter(points[y_km == 2,0], points[y_km == 2,1], s=50, color='yellow')
plt.scatter(points[y_km == 3,0], points[y_km == 3,1], s=50, color='cyan')
plt.scatter(points[y_km == 4,0], points[y_km == 4,1], s=50, color='blue')

plt.scatter(clusters[0][0], clusters[0][1], marker='*', s=200, color='black')
plt.scatter(clusters[1][0], clusters[1][1], marker='*', s=200, color='black')
plt.scatter(clusters[2][0], clusters[2][1], marker='*', s=200, color='black')
plt.scatter(clusters[3][0], clusters[3][1], marker='*', s=200, color='black')
plt.scatter(clusters[4][0], clusters[4][1], marker='*', s=200, color='black')

plt.show()