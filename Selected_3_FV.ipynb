{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYnxqRnF5GSx",
        "outputId": "1f0071ee-189a-4b00-eb4f-b43b25b615c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZADVRz95NSC"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas \n",
        "from itertools import zip_longest\n",
        "\n",
        "#Mai\n",
        "import pandas\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "import keras.preprocessing.text\n",
        "import keras.backend as K\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CTKWwz05WwR"
      },
      "outputs": [],
      "source": [
        "reader = \"/content/drive/MyDrive/S3/next_page_links - Sheet1 (1) (2).csv\"\n",
        "path = '/content/drive/MyDrive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qtiXGfX5pt6"
      },
      "outputs": [],
      "source": [
        "bookNames =[]\n",
        "bookCats = []\n",
        "bookLinks = []\n",
        "authorNames = []\n",
        "bookSummerztion=[]\n",
        "bookInfo =[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjhUJxAd5syx",
        "outputId": "a2322c48-0d45-4957-a80f-b53c845638af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%af%d8%b9%d9%86%d8%a7-%d9%86%d9%85%d8%aa-%d8%ad%d8%aa%d9%89-%d9%86%d9%86%d8%a7%d9%84-%d8%b4%d9%87%d8%a7%d8%af%d8%a9%d9%8b-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%a7%d8%ad%d9%80%d9%80%d9%80%d8%b0%d8%b1%d9%88%d8%a7-%d8%a7%d9%84%d8%b3%d9%80%d9%80%d8%ad%d9%80%d9%80%d8%b1-%d9%88%d8%a7%d9%84%d8%b3%d9%80%d9%80%d8%ad%d9%80%d8%b1%d8%a9-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d9%81%d8%aa%d8%a7%d9%88%d9%89-%d8%a7%d9%84%d8%b5%d9%8a%d8%a7%d9%85-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%a7%d9%84%d8%a3%d8%af%d9%84%d8%a9-%d9%88%d8%a7%d9%84%d8%a8%d8%b1%d8%a7%d9%87%d9%8a%d9%86-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%a7%d9%84%d8%a3%d9%85%d8%a7%d9%84%d9%8a-%d8%a7%d9%84%d9%85%d9%83%d9%8a%d8%a9-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%a3%d9%88%d8%b1%d8%a7%d9%82-%d8%a7%d9%84%d9%88%d8%b1%d8%af-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%b1%d8%b3%d8%a7%d8%a6%d9%84-%d8%a7%d9%84%d8%a3%d8%ad%d8%b2%d8%a7%d9%86-%d9%81%d9%8a-%d9%81%d9%84%d8%b3%d9%81%d8%a9-%d8%a7%d9%84%d8%ac%d9%85%d8%a7%d9%84-%d9%88%d8%a7%d9%84/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%aa%d8%ad%d8%aa-%d8%b1%d8%a7%d9%8a%d8%a9-%d8%a7%d9%84%d9%82%d8%b1%d8%a7%d9%86/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%ad%d8%af%d9%8a%d8%ab-%d8%a7%d9%84%d9%82%d9%85%d8%b1/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d9%83%d9%84%d9%85%d8%a9-%d9%88%d9%83%d9%84%d9%8a%d9%85%d8%a9/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%a5%d8%b9%d8%ac%d8%a7%d8%b2-%d8%a7%d9%84%d9%82%d8%b1%d8%a7%d9%86-%d9%88%d8%a7%d9%84%d8%a8%d9%84%d8%a7%d8%ba%d8%a9-%d8%a7%d9%84%d9%86%d8%a8%d9%88%d9%8a%d8%a9/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%a7%d9%84%d8%b3%d8%ad%d8%a7%d8%a8-%d8%a7%d9%84%d8%a3%d8%ad%d9%85%d8%b1/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%af%d9%8a%d9%88%d8%a7%d9%86-%d8%a7%d9%84%d8%b1%d8%a7%d9%81%d8%b9%d9%8a-%d8%ac2-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%af%d9%8a%d9%88%d8%a7%d9%86-%d8%a7%d9%84%d8%b1%d8%a7%d9%81%d8%b9%d9%8a-%d8%ac1-pdf-%d9%84%d9%85%d8%b5%d8%b7%d9%81%d9%89-%d8%b5%d8%a7%d8%af%d9%82-%d8%a7%d9%84%d8%b1%d8%a7/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%aa%d8%a7%d8%b1%d9%8a%d8%ae-%d8%a2%d8%af%d8%a7%d8%a8-%d8%a7%d9%84%d8%b9%d8%b1%d8%a8-%d8%ac3/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%aa%d8%a7%d8%b1%d9%8a%d8%ae-%d8%a2%d8%af%d8%a7%d8%a8-%d8%a7%d9%84%d8%b9%d8%b1%d8%a8-%d8%ac2/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%aa%d8%a7%d8%b1%d9%8a%d8%ae-%d8%a2%d8%af%d8%a7%d8%a8-%d8%a7%d9%84%d8%b9%d8%b1%d8%a8-%d8%ac1/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%a7%d9%84%d8%b3%d9%85%d9%88-%d8%a7%d9%84%d8%b1%d9%88%d8%ad%d9%8a-%d8%a7%d9%84%d8%a3%d8%b9%d8%b8%d9%85-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%b4%d8%b1%d8%ad-%d8%a7%d9%84%d8%b9%d9%82%d9%8a%d8%af%d8%a9-%d8%a7%d9%84%d9%88%d8%a7%d8%b3%d8%b7%d9%8a%d8%a9-%d8%ac1-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d9%88%d8%ad%d9%8a-%d8%a7%d9%84%d9%82%d9%84%d9%85-%d8%ac3-pdf-%d9%84%d9%85%d8%b5%d8%b7%d9%81%d9%89-%d8%b5%d8%a7%d8%af%d9%82-%d8%a7%d9%84%d8%b1%d8%a7%d9%81%d8%b9%d9%8a/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d9%88%d8%ad%d9%8a-%d8%a7%d9%84%d9%82%d9%84%d9%85-%d8%ac2-pdf-%d9%84%d9%85%d8%b5%d8%b7%d9%81%d9%89-%d8%b5%d8%a7%d8%af%d9%82-%d8%a7%d9%84%d8%b1%d8%a7%d9%81%d8%b9%d9%8a/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d9%88%d8%ad%d9%8a-%d8%a7%d9%84%d9%82%d9%84%d9%85-%d8%ac1-pdf-%d9%84%d9%85%d8%b5%d8%b7%d9%81%d9%89-%d8%b5%d8%a7%d8%af%d9%82-%d8%a7%d9%84%d8%b1%d8%a7%d9%81%d8%b9%d9%8a/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%a7%d9%84%d8%aa%d8%a8%d9%8a%d8%a7%d9%86-%d9%81%d9%8a-%d8%b4%d8%b1%d8%ad-%d9%86%d9%88%d8%a7%d9%82%d8%b6-%d8%a7%d9%84%d8%a5%d8%b3%d9%84%d8%a7%d9%85-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%a3%d8%b3%d8%a6%d9%84%d8%a9-%d8%b9%d9%86-%d8%b5%d8%ad%d8%a9-%d8%a8%d8%b9%d8%b6-%d8%a7%d9%84%d8%a3%d8%ad%d8%a7%d8%af%d9%8a%d8%ab-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%a7%d9%84%d9%81%d9%88%d8%a7%d8%a6%d8%af-%d9%88%d8%a7%d9%84%d9%85%d8%b9%d8%a7%d9%86%d9%8a-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%a8%d8%af%d8%a7%d8%a6%d8%b9-%d8%a7%d9%84%d8%ad%d9%83%d9%85/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%a7%d9%84%d9%85%d9%86%d8%aa%d9%82%d9%8a-%d9%85%d9%86-%d9%85%d8%b1%d8%a7%d9%82%d9%8a-%d8%a7%d9%84%d8%b3%d8%b9%d9%88%d8%af-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d9%85%d9%86-%d9%82%d8%b5%d8%b5-%d8%a7%d9%84%d8%aa%d8%a7%d8%b1%d9%8a%d8%ae-pdf-%d9%84%d9%85%d8%b5%d8%b7%d9%81%d9%89-%d8%b5%d8%a7%d8%af%d9%82-%d8%a7%d9%84%d8%b1%d8%a7%d9%81/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d9%85%d9%87%d9%85%d8%a7%d8%aa-%d8%a7%d9%84%d9%85%d8%b3%d8%a7%d8%a6%d9%84-pdf/', 'https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d9%85%d8%ac%d8%a7%d9%84%d8%b3-%d8%a8%d9%84%d8%ae-pdf-%d9%84%d9%85%d8%b5%d8%b7%d9%81%d9%89-%d8%b5%d8%a7%d8%af%d9%82-%d8%a7%d9%84%d8%b1%d8%a7%d9%81%d8%b9%d9%8a/', 'https://www.arab-books.com/%d9%85%d9%88%d8%b9%d8%af-%d8%b9%d9%8a%d8%af-%d8%a7%d9%84%d8%a3%d8%b6%d8%ad%d9%89-%d9%81%d9%8a-%d8%a7%d9%84%d8%b3%d9%88%d9%8a%d8%af-2021/', 'https://www.arab-books.com/%d9%85%d9%88%d8%b9%d8%af-%d8%b9%d9%8a%d8%af-%d8%a7%d9%84%d8%a3%d8%b6%d8%ad%d9%89-%d9%81%d9%8a-%d8%a7%d9%84%d9%85%d8%a7%d9%86%d9%8a%d8%a7-2021/', 'https://www.arab-books.com/%d9%85%d9%88%d8%b9%d8%af-%d8%b9%d9%8a%d8%af-%d8%a7%d9%84%d8%a3%d8%b6%d8%ad%d9%89-%d9%81%d9%8a-%d8%aa%d8%b1%d9%83%d9%8a%d8%a7-2021/', 'https://www.arab-books.com/%d9%85%d9%88%d8%b9%d8%af-%d8%b9%d9%8a%d8%af-%d8%a7%d9%84%d8%a3%d8%b6%d8%ad%d9%89-%d9%81%d9%8a-%d8%b1%d9%88%d8%b3%d9%8a%d8%a7-2021/', 'https://www.arab-books.com/%d9%85%d9%88%d8%b9%d8%af-%d8%b9%d9%8a%d8%af-%d8%a7%d9%84%d8%a3%d8%b6%d8%ad%d9%89-%d9%81%d9%8a-%d8%a7%d9%84%d8%b3%d9%88%d9%8a%d8%b3-2021/', 'https://www.arab-books.com/%d9%85%d9%88%d8%b9%d8%af-%d8%b9%d9%8a%d8%af-%d8%a7%d9%84%d8%a7%d8%b6%d8%ad%d9%89-2021-%d8%a7%d9%84%d8%af%d9%82%d9%87%d9%84%d9%8a%d8%a9-%d9%84%d8%b9%d8%a7%d9%85-1442-%d9%87%d8%ac%d8%b1%d9%8a/', 'https://www.arab-books.com/%d9%85%d9%88%d8%b9%d8%af-%d8%b9%d9%8a%d8%af-%d8%a7%d9%84%d8%a7%d8%b6%d8%ad%d9%89-2021-%d9%81%d8%b1%d9%86%d8%b3%d8%a7-%d9%84%d8%b9%d8%a7%d9%85-1442-%d9%87%d8%ac%d8%b1%d9%8a/', 'https://www.arab-books.com/%d9%85%d9%88%d8%b9%d8%af-%d8%b9%d9%8a%d8%af-%d8%a7%d9%84%d8%a7%d8%b6%d8%ad%d9%89-2021-%d8%a7%d9%84%d8%a7%d9%82%d8%b5%d8%b1-%d9%84%d8%b9%d8%a7%d9%85-1442-%d9%87%d8%ac%d8%b1%d9%8a/']\n",
            "['كتاب دعنا نمت حتى ننال شهادةً PDF'] ['كتب اسلامية'] ['\\nالكاتب سليمان بن ناصر العلوان\\n'] ['https://www.arab-books.com/books/%d9%83%d8%aa%d8%a7%d8%a8-%d8%af%d8%b9%d9%86%d8%a7-%d9%86%d9%85%d8%aa-%d8%ad%d8%aa%d9%89-%d9%86%d9%86%d8%a7%d9%84-%d8%b4%d9%87%d8%a7%d8%af%d8%a9%d9%8b-pdf/?view=download'] ['\\n\\n \\n\\n\\nمؤلف الكتاب:\\nكتب الكاتب سليمان بن ناصر العلوان\\n\\nقسم الكتاب: \\nتحميل كتب اسلامية\\n\\nلغة الكتاب: عربي\\nعدد الصّفحات: 13 صفحة\\nدار النشر: دار الإيمان للنشر والتوزيع\\nحجم الكتاب: 1.66 ميغابايت\\nملف الكتاب: pdf\\nتبليغ حقوق الملكية: اضغط هنا\\n\\n\\n'] ['\\n\\n\\n\\r\\n     (adsbygoogle = window.adsbygoogle || []).push({});\\r\\n\\nملخص كتاب دعنا نمت حتى ننال شهادةً PDF\\n\\n\\r\\n     (adsbygoogle = window.adsbygoogle || []).push({});\\r\\n\\nتلخيص كتاب دعنا نمت حتى ننال شهادةً PDF للكاتب سليمان بن ناصر العلوان\\n\\nكتاب دعنا نمت حتى ننال شهادةً PDF \\xa0 للكاتب سليمان بن ناصر العلواني استخدم فيه الكاتب سليمان بن ناصر أساليب ومعاني واضحة لتيسير الفهم على القراء وتوصيل جميع المفاهيم بطريقة واضحة لإفادة جميع الفئات من قراءة كتبه وسيتضح لكم ذلك من خلال قرائتكم لكل كتاب سنقوم بنشره لكم.\\nإن الدين الإسلامي لا يتحقق في أنفس المسلمين، ولا في واقع الناس إلا بإقامة الجهاد في سبيل الله بجميع أنواعه، ولا ينقمع شر المفسدين في الأرض إلا بقوة ترهبهم وجهاد يكسر شوكتهم.\\n\\n\\nكتاب دعنا نمت حتى ننال شهادةً PDF\\nنبذة عن الكاتب\\xa0سليمان بن ناصر العلوان\\n\\nسليمان العلوان\\xa0واسمه الكامل هو\\xa0سليمان بن ناصر بن عبد الله العلوان\\xa0ولديه العديد من المؤلفات والكتب، اعتقل في شهر\\xa0أبريل\\xa02004\\xa0الموافق\\xa0ربيع الأول\\xa01425 هـ\\xa0بتهمة دعم\\xa0الجماعات الجهادية\\xa0واطلق سراحه في\\xa02012\\xa0وأعيد اعتقاله في\\xa0أكتوبر\\xa02013\\xa0بتهم كثيرة، في (2014\\xa0–\\xa02015) ظهرت دعوات من ذويه ومؤيديه في\\xa0مواقع التواصل الاجتماعي\\xa0السعودية بإطلاق سراحه.\\n\\nمؤلفات للكاتب سليمان بن ناصر العلوان\\n\\nتنبيه الأخيار على عدم فناء النار.\\nالأمالي المكية على المنظومة البيقونية.\\nالتبيان في شرح نواقض الإسلام.\\nشرح بلوغ المرام.\\nتنبيه الأمة على وجوب الأخذ بالكتاب والسنة.\\nالتوكيد في وجوب الاعتناء بالتوحيد.\\n\\nاقتباسات من كتاب دعنا نمت حتى ننال شهادةً PDF للكاتب سليمان بن ناصر العلوان:\\n\\nوفي سورة براءة جعل الله الجنة ثمناً لنفوس المؤمنين وأموالهم فقال: ﴿إِنَّ اللَّهَ اشْتَرَى مِنْ الْمُؤْمِنِينَ أَنفُسَهُمْ وَأَمْوَالَهُمْ بِأَنَّ لَهُمْ الْجَنَّةَ يُقَاتِلُونَ فِي سَبِيلِ اللَّهِ فَيَقْتُلُونَ وَيُقْتَلُونَ وَعْداً عَلَيْهِ حَقّاً فِي التَّوْرَاةِ وَالإِنجِيلِ وَالْقُرْآنِ وَمَنْ أَوْفَى بِعَهْدِهِ مِنْ اللَّهِ فَاسْتَبْشِرُوا بِبَيْعِكُمْ الَّذِي بَايَعْتُمْ بِهِ وَذَلِكَ هُوَ الْفَوْزُ الْعَظِيمُ﴾ [التوبة: 111].\\nوقد أجمع العلماء على وجوب قتال الكفار المعتدين على بلاد المسلمين، فإن اندفع شرهم بأهل تلك البلاد المحتلة أو المغتصبة فيكفي ذلك عن غيرهم، وإن لم يحصل رَدُّ كيدهم وعدوانهم بأهل تلك البلاد المحتلة، فإنه يجب على مَنْ قرب من العدو من أهل البلاد الأخرى مناصرة إخوانهم وصد عدوان الكافرين، ولا يسقط الوجوب عن المسلمين حتى يُطرد العدو من بلاد المسلمين.\\n\\nكتاب دعنا نمت حتى ننال شهادةً PDF\\nمن خلال مكتبتكم المكتبة العربية للكتب يمكنكم تحميل وقراءة:\\n\\nكتب إسلامية\\nكتب روايات عربية\\nكتب الأدب العربي\\n\\n\\n\\r\\n     (adsbygoogle = window.adsbygoogle || []).push({});\\r\\n\\n']\n"
          ]
        }
      ],
      "source": [
        "result = requests.get(\"https://www.arab-books.com/\")\n",
        "src = result.content\n",
        "soup = BeautifulSoup(src,\"lxml\")\n",
        "book_links = soup.find_all(\"h3\",{\"class\": \"post-title\"})\n",
        "book_links = str(book_links)\n",
        "x = re.findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+', book_links)\n",
        "author = soup.find_all(\"div\",{\"class\": \"book-writer\"})\n",
        "#print(author)\n",
        "#print(type(x))\n",
        "print(x) \n",
        "book = requests.get(x[0])\n",
        "book_content = book.content\n",
        "soup_ = BeautifulSoup(book_content,\"lxml\")\n",
        "book_cat = soup_.find_all(\"span\" , {\"class\": \"post-cat-wrap\"})\n",
        "book_name = soup_.find_all(\"h1\" , {\"class\": \"post-title entry-title\"})\n",
        "link = soup_.find_all(\"div\" , {\"class\": \"down-link-bottom\"})\n",
        "link = str(link)\n",
        "pdf_link = re.findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+', link)\n",
        "book_Info = soup_.find_all(\"div\",{\"class\": \"book-infos\"})\n",
        "book_Summerztion = soup_.find_all(\"div\",{\"class\": \"entry-content entry clearfix\"})\n",
        "\n",
        "for i in range(len(book_name)):\n",
        "  bookNames.append(book_name[i].text)\n",
        "  bookCats.append(book_cat[i].text)\n",
        "  authorNames.append(author[i].text)\n",
        "  bookLinks.append(pdf_link[i])\n",
        "  bookInfo.append(book_Info[i].text)\n",
        "  bookSummerztion.append(book_Summerztion[i].text)\n",
        "  \n",
        "print(bookNames,bookCats,authorNames,bookLinks,bookInfo,bookSummerztion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD3NEd7354eo"
      },
      "outputs": [],
      "source": [
        "links = pandas.read_csv(reader)\n",
        "#links.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_D-2fc158R4",
        "outputId": "4f8c0bbe-e9fa-46eb-a395-4f168d582817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['https://www.arab-books.com/' 'https://www.arab-books.com//page/2'\n",
            " 'https://www.arab-books.com//page/3' 'https://www.arab-books.com//page/4'\n",
            " 'https://www.arab-books.com//page/5' 'https://www.arab-books.com//page/6'\n",
            " 'https://www.arab-books.com//page/7' 'https://www.arab-books.com//page/8'\n",
            " 'https://www.arab-books.com//page/9'\n",
            " 'https://www.arab-books.com//page/10'\n",
            " 'https://www.arab-books.com//page/11'\n",
            " 'https://www.arab-books.com//page/12'\n",
            " 'https://www.arab-books.com//page/13'\n",
            " 'https://www.arab-books.com//page/14'\n",
            " 'https://www.arab-books.com//page/15'\n",
            " 'https://www.arab-books.com//page/16'\n",
            " 'https://www.arab-books.com//page/17'\n",
            " 'https://www.arab-books.com//page/18'\n",
            " 'https://www.arab-books.com//page/19'\n",
            " 'https://www.arab-books.com//page/20'\n",
            " 'https://www.arab-books.com//page/21'\n",
            " 'https://www.arab-books.com//page/22'\n",
            " 'https://www.arab-books.com//page/23'\n",
            " 'https://www.arab-books.com//page/24'\n",
            " 'https://www.arab-books.com//page/25'\n",
            " 'https://www.arab-books.com//page/26'\n",
            " 'https://www.arab-books.com//page/27'\n",
            " 'https://www.arab-books.com//page/28'\n",
            " 'https://www.arab-books.com//page/29'\n",
            " 'https://www.arab-books.com//page/30'\n",
            " 'https://www.arab-books.com//page/31'\n",
            " 'https://www.arab-books.com//page/32'\n",
            " 'https://www.arab-books.com//page/33'\n",
            " 'https://www.arab-books.com//page/34'\n",
            " 'https://www.arab-books.com//page/35'\n",
            " 'https://www.arab-books.com//page/36'\n",
            " 'https://www.arab-books.com//page/37'\n",
            " 'https://www.arab-books.com//page/38'\n",
            " 'https://www.arab-books.com//page/39'\n",
            " 'https://www.arab-books.com//page/40'\n",
            " 'https://www.arab-books.com//page/41'\n",
            " 'https://www.arab-books.com//page/42'\n",
            " 'https://www.arab-books.com//page/43'\n",
            " 'https://www.arab-books.com//page/44'\n",
            " 'https://www.arab-books.com//page/45'\n",
            " 'https://www.arab-books.com//page/46'\n",
            " 'https://www.arab-books.com//page/47'\n",
            " 'https://www.arab-books.com//page/48'\n",
            " 'https://www.arab-books.com//page/49'\n",
            " 'https://www.arab-books.com//page/50'\n",
            " 'https://www.arab-books.com//page/51'\n",
            " 'https://www.arab-books.com//page/52'\n",
            " 'https://www.arab-books.com//page/53'\n",
            " 'https://www.arab-books.com//page/54'\n",
            " 'https://www.arab-books.com//page/55'\n",
            " 'https://www.arab-books.com//page/56'\n",
            " 'https://www.arab-books.com//page/57'\n",
            " 'https://www.arab-books.com//page/58'\n",
            " 'https://www.arab-books.com//page/59'\n",
            " 'https://www.arab-books.com//page/60'\n",
            " 'https://www.arab-books.com//page/61'\n",
            " 'https://www.arab-books.com//page/62'\n",
            " 'https://www.arab-books.com//page/63'\n",
            " 'https://www.arab-books.com//page/64'\n",
            " 'https://www.arab-books.com//page/65'\n",
            " 'https://www.arab-books.com//page/66'\n",
            " 'https://www.arab-books.com//page/67'\n",
            " 'https://www.arab-books.com//page/68'\n",
            " 'https://www.arab-books.com//page/69'\n",
            " 'https://www.arab-books.com//page/70'\n",
            " 'https://www.arab-books.com//page/71'\n",
            " 'https://www.arab-books.com//page/72'\n",
            " 'https://www.arab-books.com//page/73'\n",
            " 'https://www.arab-books.com//page/74'\n",
            " 'https://www.arab-books.com//page/75'\n",
            " 'https://www.arab-books.com//page/76'\n",
            " 'https://www.arab-books.com//page/77'\n",
            " 'https://www.arab-books.com//page/78'\n",
            " 'https://www.arab-books.com//page/79'\n",
            " 'https://www.arab-books.com//page/80'\n",
            " 'https://www.arab-books.com//page/81'\n",
            " 'https://www.arab-books.com//page/82'\n",
            " 'https://www.arab-books.com//page/83'\n",
            " 'https://www.arab-books.com//page/84'\n",
            " 'https://www.arab-books.com//page/85'\n",
            " 'https://www.arab-books.com//page/86'\n",
            " 'https://www.arab-books.com//page/87'\n",
            " 'https://www.arab-books.com//page/88'\n",
            " 'https://www.arab-books.com//page/89'\n",
            " 'https://www.arab-books.com//page/90'\n",
            " 'https://www.arab-books.com//page/91'\n",
            " 'https://www.arab-books.com//page/92'\n",
            " 'https://www.arab-books.com//page/93'\n",
            " 'https://www.arab-books.com//page/94'\n",
            " 'https://www.arab-books.com//page/95'\n",
            " 'https://www.arab-books.com//page/96'\n",
            " 'https://www.arab-books.com//page/97'\n",
            " 'https://www.arab-books.com//page/98'\n",
            " 'https://www.arab-books.com//page/99'\n",
            " 'https://www.arab-books.com//page/100'\n",
            " 'https://www.arab-books.com//page/101'\n",
            " 'https://www.arab-books.com//page/102'\n",
            " 'https://www.arab-books.com//page/103'\n",
            " 'https://www.arab-books.com//page/104'\n",
            " 'https://www.arab-books.com//page/105'\n",
            " 'https://www.arab-books.com//page/106'\n",
            " 'https://www.arab-books.com//page/107'\n",
            " 'https://www.arab-books.com//page/108'\n",
            " 'https://www.arab-books.com//page/109'\n",
            " 'https://www.arab-books.com//page/110'\n",
            " 'https://www.arab-books.com//page/111'\n",
            " 'https://www.arab-books.com//page/112'\n",
            " 'https://www.arab-books.com//page/113'\n",
            " 'https://www.arab-books.com//page/114'\n",
            " 'https://www.arab-books.com//page/115'\n",
            " 'https://www.arab-books.com//page/116'\n",
            " 'https://www.arab-books.com//page/117'\n",
            " 'https://www.arab-books.com//page/118'\n",
            " 'https://www.arab-books.com//page/119'\n",
            " 'https://www.arab-books.com//page/120'\n",
            " 'https://www.arab-books.com//page/121'\n",
            " 'https://www.arab-books.com//page/122'\n",
            " 'https://www.arab-books.com//page/123'\n",
            " 'https://www.arab-books.com//page/124'\n",
            " 'https://www.arab-books.com//page/125'\n",
            " 'https://www.arab-books.com//page/126'\n",
            " 'https://www.arab-books.com//page/127'\n",
            " 'https://www.arab-books.com//page/128'\n",
            " 'https://www.arab-books.com//page/129'\n",
            " 'https://www.arab-books.com//page/130'\n",
            " 'https://www.arab-books.com//page/131'\n",
            " 'https://www.arab-books.com//page/132'\n",
            " 'https://www.arab-books.com//page/133'\n",
            " 'https://www.arab-books.com//page/134'\n",
            " 'https://www.arab-books.com//page/135'\n",
            " 'https://www.arab-books.com//page/136'\n",
            " 'https://www.arab-books.com//page/137'\n",
            " 'https://www.arab-books.com//page/138'\n",
            " 'https://www.arab-books.com//page/139'\n",
            " 'https://www.arab-books.com//page/140'\n",
            " 'https://www.arab-books.com//page/141'\n",
            " 'https://www.arab-books.com//page/142'\n",
            " 'https://www.arab-books.com//page/143'\n",
            " 'https://www.arab-books.com//page/144'\n",
            " 'https://www.arab-books.com//page/145'\n",
            " 'https://www.arab-books.com//page/146'\n",
            " 'https://www.arab-books.com//page/147'\n",
            " 'https://www.arab-books.com//page/148'\n",
            " 'https://www.arab-books.com//page/149'\n",
            " 'https://www.arab-books.com//page/150'\n",
            " 'https://www.arab-books.com//page/151'\n",
            " 'https://www.arab-books.com//page/152'\n",
            " 'https://www.arab-books.com//page/153'\n",
            " 'https://www.arab-books.com//page/154'\n",
            " 'https://www.arab-books.com//page/155'\n",
            " 'https://www.arab-books.com//page/156'\n",
            " 'https://www.arab-books.com//page/157'\n",
            " 'https://www.arab-books.com//page/158'\n",
            " 'https://www.arab-books.com//page/159'\n",
            " 'https://www.arab-books.com//page/160'\n",
            " 'https://www.arab-books.com//page/161'\n",
            " 'https://www.arab-books.com//page/162'\n",
            " 'https://www.arab-books.com//page/163'\n",
            " 'https://www.arab-books.com//page/164'\n",
            " 'https://www.arab-books.com//page/165'\n",
            " 'https://www.arab-books.com//page/166'\n",
            " 'https://www.arab-books.com//page/167'\n",
            " 'https://www.arab-books.com//page/168'\n",
            " 'https://www.arab-books.com//page/169'\n",
            " 'https://www.arab-books.com//page/170'\n",
            " 'https://www.arab-books.com//page/171'\n",
            " 'https://www.arab-books.com//page/172'\n",
            " 'https://www.arab-books.com//page/173'\n",
            " 'https://www.arab-books.com//page/174'\n",
            " 'https://www.arab-books.com//page/175'\n",
            " 'https://www.arab-books.com//page/176'\n",
            " 'https://www.arab-books.com//page/177'\n",
            " 'https://www.arab-books.com//page/178'\n",
            " 'https://www.arab-books.com//page/179'\n",
            " 'https://www.arab-books.com//page/180'\n",
            " 'https://www.arab-books.com//page/181'\n",
            " 'https://www.arab-books.com//page/182'\n",
            " 'https://www.arab-books.com//page/183'\n",
            " 'https://www.arab-books.com//page/184'\n",
            " 'https://www.arab-books.com//page/185'\n",
            " 'https://www.arab-books.com//page/186'\n",
            " 'https://www.arab-books.com//page/187'\n",
            " 'https://www.arab-books.com//page/188'\n",
            " 'https://www.arab-books.com//page/189'\n",
            " 'https://www.arab-books.com//page/190'\n",
            " 'https://www.arab-books.com//page/191'\n",
            " 'https://www.arab-books.com//page/192'\n",
            " 'https://www.arab-books.com//page/193'\n",
            " 'https://www.arab-books.com//page/194'\n",
            " 'https://www.arab-books.com//page/195'\n",
            " 'https://www.arab-books.com//page/196'\n",
            " 'https://www.arab-books.com//page/197'\n",
            " 'https://www.arab-books.com//page/198'\n",
            " 'https://www.arab-books.com//page/199'\n",
            " 'https://www.arab-books.com//page/200']\n"
          ]
        }
      ],
      "source": [
        "next_page_link = links['page links'].values\n",
        "print(next_page_link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Div3b1836ExQ",
        "outputId": "6c200e11-5fcc-423a-bb77-e8016f16896b"
      },
      "outputs": [],
      "source": [
        "for i in range(0,len(next_page_link)):\n",
        "    result = requests.get(next_page_link[i],allow_redirects=False)\n",
        "    src = result.content\n",
        "    soup = BeautifulSoup(src,\"lxml\")\n",
        "    book_links = soup.find_all(\"h3\",{\"class\": \"post-title\"})\n",
        "    book_links = str(book_links)\n",
        "    x = re.findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+', book_links)\n",
        "    author = soup.find_all(\"div\",{\"class\": \"book-writer\"})\n",
        "    #print(type(x))\n",
        "    #print(x)\n",
        "    for j in range(0,len(x)):\n",
        "        #print(x)\n",
        "      book = requests.get(x[j],allow_redirects=False)\n",
        "      book_content = book.content\n",
        "      soup_ = BeautifulSoup(book_content,\"lxml\")\n",
        "      book_cat = soup_.find_all(\"span\" , {\"class\": \"post-cat-wrap\"})\n",
        "      book_name = soup_.find_all(\"h1\" , {\"class\": \"post-title entry-title\"})\n",
        "      link = soup_.find_all(\"div\" , {\"class\": \"down-link-bottom\"})\n",
        "      link = str(link)\n",
        "      pdf_link = re.findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+', link)\n",
        "      book_Info = soup_.find_all(\"div\",{\"class\": \"book-infos\"})\n",
        "      book_Summerztion = soup_.find_all(\"div\",{\"class\": \"entry-content entry clearfix\"})\n",
        "\n",
        "      #print(book_cat,book_name,pdf_link)\n",
        "      for m in range(len(pdf_link)):\n",
        "        bookLinks.append(pdf_link[m])\n",
        "      for n in  range(len(book_name)):\n",
        "        bookNames.append(book_name[n].text)\n",
        "        bookCats.append(book_cat[n].text)\n",
        "        authorNames.append(author[n].text)\n",
        "      for z in range(len(book_Info)):\n",
        "          bookInfo.append(book_Info[z].text)\n",
        "          bookSummerztion.append(book_Summerztion[z].text)\n",
        "  \n",
        "      print(bookNames,bookCats,authorNames,bookLinks,bookInfo,bookSummerztion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BwacN_MXCzI"
      },
      "outputs": [],
      "source": [
        "file_list = [bookNames,bookCats,authorNames,bookLinks,bookInfo,bookSummerztion]\n",
        "exported = zip_longest(*file_list)\n",
        "with open(\"/content/drive/MyDrive/S3_phase2/Scrapped_Data.csv\", \"w\") as myfile: \n",
        "\n",
        "\twr = csv.writer(myfile)\n",
        "\twr.writerow(['اسم الكاتب', 'عنوان الكتاب', 'نوع الكتاب', 'رابط الكتاب','معلومات عن الكتاب','ملخص الكتاب'])\n",
        "\twr.writerows(exported)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApzbNo5Fs_qD",
        "outputId": "7e510371-2cb9-45a4-dc76-e16077252947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "header = ['اسم الكاتب', 'عنوان الكتاب', 'نوع الكتاب', 'رابط الكتاب','معلومات عن الكتاب','ملخص الكتاب']\n",
        "extracted_data_file     = '/content/drive/MyDrive/S3_phase2/Scrapped_Data.csv'\n",
        "cleaned_data_file       = '/content/drive/MyDrive/S3_phase2/data_modified.csv'\n",
        "autocorrected_data_file = '/content/drive/MyDrive/S3_phase2/data_autocorrected.csv'\n",
        "dictionary_file         = '/content/drive/MyDrive/S3_phase2/arabic-wordlist-1.6.txt'\n",
        "delimiter = '،،،،'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/S3_phase2/Scrapped_Data.csv\"\n",
        "path = '/content/drive/MyDrive'\n",
        "# ---------------------------------------------------------------------------- #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzbcuhIttNW1"
      },
      "outputs": [],
      "source": [
        "### Step 2) Data_preprocessing\n",
        "\n",
        "class preprocessing(object):\n",
        "    \n",
        "    def __init__(self, authors, titles, categories, links,Summerztion,Info):\n",
        "        \n",
        "        modified_authors    = self.Data_preprocessing(authors)\n",
        "        modified_titles     = self.Data_preprocessing(titles)\n",
        "        modified_categories = self.Data_preprocessing(categories)\n",
        "        modified_summerztion =self.Data_preprocessing(Summerztion)\n",
        "        modified_info =self.Data_preprocessing(Info)\n",
        "        ## Write Cleaned Scrapped Data Into New Text File\n",
        "        #save modified Data In csv File \n",
        "        with open(cleaned_data_file, \"w\", encoding='utf-8-sig') as file:\n",
        "            wr = csv.writer(file)\n",
        "            wr.writerow(header)\n",
        "            colum1 = modified_authors.split(delimiter)\n",
        "            colum2 = modified_titles.split(delimiter)\n",
        "            colum3 = modified_categories.split(delimiter)\n",
        "            colum4 = modified_summerztion.split(delimiter)\n",
        "            colum5 = modified_info.split(delimiter)\n",
        "            \n",
        "            for (l1, l2, l3, l4) in zip(colum1, colum2, colum3,colum4,colum5, links):\n",
        "                s = u','.join([str(l1), str(l2), str(l3), str(l4)]) + u'\\n'\n",
        "                file.write(s)\n",
        "                \n",
        "          \n",
        "      \n",
        "\n",
        "    def Data_preprocessing(self, text):\n",
        "        ## 1.1 Replace punctuations with a white space\n",
        "        remove_punctuations = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "        ## 1.2 Normalize\n",
        "        normalized = re.sub(\"گ\", \"ك\", remove_punctuations)\n",
        "        normalized = re.sub(\"ى\", \"ي\", normalized)\n",
        "        ## 2.1 Remove Non-Arabic Words\n",
        "        remove_nonarabic = re.sub(r'\\s*[A-Za-z]+\\b', ' ' , normalized)\n",
        "        ## 2.2 Remove Stop Words.\n",
        "        stop_words = set(stopwords.words('arabic'))\n",
        "        filtered_sentence = ' '.join([word for word in remove_nonarabic.split()if word not in stop_words]) \n",
        "        ## 3.0 Stemming\n",
        "        st = ISRIStemmer()\n",
        "        st.stem(remove_nonarabic)\n",
        "        \n",
        "        return remove_nonarabic\n",
        "    \n",
        "# ---------------------------------------------------------------------------- #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuD-LHzatc8N",
        "outputId": "b82e530f-ab19-4a35-fd13-100a60175992"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f04f8c441b11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSummerztion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mauthors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSummerztion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_data_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mauthors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-f04f8c441b11>\u001b[0m in \u001b[0;36msplit_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# converting columns data to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "### preprocessing\n",
        "def split_file(filename):\n",
        "    data = pandas.read_csv(data_path)\n",
        "    \n",
        "    # converting columns data to list\n",
        "    authors = data['اسم الكاتب'].tolist()\n",
        "    titles = data['عنوان الكتاب'].tolist()\n",
        "    categories = data['نوع الكتاب'].tolist()\n",
        "    links = data['رابط الكتاب'].tolist()\n",
        "    Summerztion = data['ملخص الكتاب'].tolist()\n",
        "    Info = data['معلومات عن الكتاب'].tolist()\n",
        "\n",
        "\n",
        "    return authors, titles, categories, links,Summerztion,Info\n",
        "\n",
        "authors, titles, categories, links,Summerztion,Info = split_file(extracted_data_file)\n",
        "\n",
        "authors = list(map(lambda s: s+delimiter, authors))\n",
        "titles = list(map(lambda s: s+delimiter, titles))\n",
        "categories = list(map(lambda s: s+delimiter, categories))\n",
        "Summerztion =list(map(lambda s: s+delimiter,Summerztion))\n",
        "Info =list(map(lambda s: s+delimiter,Info))\n",
        "\n",
        "\n",
        "preprocessing(str(authors), str(titles), str(categories),str(Summerztion),str(Info), links)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STmwAiks-eDN",
        "outputId": "04a87765-73f3-4eb0-c176-889c3ad252a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting AutoCorrecting Misspelling....\n",
            "\n",
            "\n",
            "Suggested Words: [('الكاتب', 0.00641025641025641)]\n",
            "\n",
            "'الكاتب' is Suggested for '\n",
            "الكاتب'\n",
            "\n",
            "Suggested Words: [('الكتاب', 0.016025641025641024)]\n",
            "\n",
            "'الكتاب' is Suggested for 'الكتاب:'\n",
            "\n",
            "Suggested Words: [('تحميل', 0.003205128205128205)]\n",
            "\n",
            "'تحميل' is Suggested for '\n",
            "تحميل'\n",
            "\n",
            "Suggested Words: [('الكتاب', 0.016025641025641024)]\n",
            "\n",
            "'الكتاب' is Suggested for 'الكتاب:'\n",
            "\n",
            "Suggested Words: [('النشر', 0.0016025641025641025)]\n",
            "\n",
            "'النشر' is Suggested for 'النشر:'\n",
            "\n",
            "Suggested Words: [('الكتاب', 0.016025641025641024)]\n",
            "\n",
            "'الكتاب' is Suggested for 'الكتاب:'\n",
            "\n",
            "Suggested Words: [('الكتاب', 0.016025641025641024)]\n",
            "\n",
            "'الكتاب' is Suggested for 'الكتاب:'\n",
            "\n",
            "Suggested Words: [('الملكية', 0.0016025641025641025)]\n",
            "\n",
            "'الملكية' is Suggested for 'الملكية:'\n",
            "\n",
            "Suggested Words: [('adsbygoogle', 0.01282051282051282)]\n",
            "\n",
            "'adsbygoogle' is Suggested for '(adsbygoogle'\n",
            "\n",
            "Suggested Words: [('adsbygoogle', 0.01282051282051282)]\n",
            "\n",
            "'adsbygoogle' is Suggested for '(adsbygoogle'\n",
            "\n",
            "Suggested Words: [('البخاري', 0.016025641025641024)]\n",
            "\n",
            "'البخاري' is Suggested for 'البخاري '\n",
            "\n",
            "Suggested Words: [('أبوابا', 0.0016025641025641025)]\n",
            "\n",
            "'أبوابا' is Suggested for 'أبواباً'\n",
            "\n",
            "Suggested Words: [('ترتيبا', 0.0016025641025641025)]\n",
            "\n",
            "'ترتيبا' is Suggested for 'ترتيباً'\n",
            "\n",
            "Suggested Words: [('متقا', 0.0016025641025641025)]\n",
            "\n",
            "'متقا' is Suggested for 'متقاً'\n",
            "\n",
            "Suggested Words: [('يعد', 0.003205128205128205)]\n",
            "\n",
            "'يعد' is Suggested for 'يعدّ'\n",
            "\n",
            "Suggested Words: [('مصدرا', 0.0016025641025641025)]\n",
            "\n",
            "'مصدرا' is Suggested for 'مصدراً'\n",
            "\n",
            "Suggested Words: [('حافلا', 0.0016025641025641025)]\n",
            "\n",
            "'حافلا' is Suggested for 'حافلاً'\n",
            "\n",
            "Suggested Words: [('المباركة', 0.0016025641025641025)]\n",
            "\n",
            "'المباركة' is Suggested for 'المباركة،'\n",
            "\n",
            "Suggested Words: [('والأعلام', 0.0016025641025641025)]\n",
            "\n",
            "'والأعلام' is Suggested for 'والأعلام؛'\n",
            "\n",
            "Suggested Words: [('13', 0.0016025641025641025)]\n",
            "\n",
            "'13' is Suggested for '(13'\n",
            "\n",
            "Suggested Words: [('هـ', 0.003205128205128205)]\n",
            "\n",
            "'هـ' is Suggested for 'هـ)'\n",
            "\n",
            "Suggested Words: [('20', 0.0016025641025641025)]\n",
            "\n",
            "'20' is Suggested for '(20'\n",
            "\n",
            "Suggested Words: [('الصحيح', 0.003205128205128205)]\n",
            "\n",
            "'الصحيح' is Suggested for 'الصحيح،'\n",
            "\n",
            "Suggested Words: [('البخاري', 0.016025641025641024)]\n",
            "\n",
            "'البخاري' is Suggested for 'البخاري،'\n",
            "\n",
            "Suggested Words: [('يتيما', 0.0016025641025641025)]\n",
            "\n",
            "'يتيما' is Suggested for 'يتيماً'\n",
            "\n",
            "Suggested Words: [('شيخ', 0.0016025641025641025)]\n",
            "\n",
            "'شيخ' is Suggested for 'شيخ،'\n",
            "\n",
            "Suggested Words: [('حديث', 0.0016025641025641025)]\n",
            "\n",
            "'حديث' is Suggested for 'حديث.'\n",
            "\n",
            "Suggested Words: [('اشتهر', 0.0016025641025641025)]\n",
            "\n",
            "'اشتهر' is Suggested for '\n",
            "اشتهر'\n",
            "\n",
            "Suggested Words: [('وأقر', 0.0016025641025641025)]\n",
            "\n",
            "'وأقر' is Suggested for 'وأقرّ'\n",
            "\n",
            "Suggested Words: [('وعلومه', 0.0016025641025641025)]\n",
            "\n",
            "'وعلومه' is Suggested for 'وعلومه،'\n",
            "\n",
            "Suggested Words: [('حتى', 0.0016025641025641025)]\n",
            "\n",
            "'حتى' is Suggested for 'حتّى'\n",
            "\n",
            "Suggested Words: [('الحديث', 0.008012820512820512)]\n",
            "\n",
            "'الحديث' is Suggested for 'الحديث.'\n",
            "\n",
            "Suggested Words: [('وتتلمذ', 0.0016025641025641025)]\n",
            "\n",
            "'وتتلمذ' is Suggested for '\n",
            "وتتلمذ'\n",
            "\n",
            "Suggested Words: [('وغيرهم', 0.0016025641025641025)]\n",
            "\n",
            "'وغيرهم' is Suggested for 'وغيرهم،'\n",
            "\n",
            "Suggested Words: [('كتابا', 0.0016025641025641025)]\n",
            "\n",
            "'كتابا' is Suggested for 'كتاباً'\n",
            "\n",
            "Suggested Words: [('الصحيح', 0.003205128205128205)]\n",
            "\n",
            "'الصحيح' is Suggested for 'الصحيح.'\n",
            "\n",
            "Suggested Words: [('ومن', 0.00641025641025641)]\n",
            "\n",
            "'ومن' is Suggested for '\n",
            "ومن'\n",
            "\n",
            "Suggested Words: [('أول', 0.0016025641025641025)]\n",
            "\n",
            "'أول' is Suggested for 'أوّل'\n",
            "\n",
            "Suggested Words: [('ألف', 0.003205128205128205)]\n",
            "\n",
            "'ألف' is Suggested for 'ألّف'\n",
            "\n",
            "Suggested Words: [('الرجال', 0.003205128205128205)]\n",
            "\n",
            "'الرجال' is Suggested for 'الرجال.'\n",
            "\n",
            "Suggested Words: [('أرضا', 0.0016025641025641025)]\n",
            "\n",
            "'أرضا' is Suggested for 'أرضاً'\n",
            "\n",
            "Suggested Words: [('adsbygoogle', 0.01282051282051282)]\n",
            "\n",
            "'adsbygoogle' is Suggested for '(adsbygoogle'\n",
            "\n",
            "Suggested Words: [('مباشرة', 0.0016025641025641025)]\n",
            "\n",
            "'مباشرة' is Suggested for 'مباشرةً'\n",
            "\n",
            "Suggested Words: [('adsbygoogle', 0.01282051282051282)]\n",
            "\n",
            "'adsbygoogle' is Suggested for '(adsbygoogle'\n",
            "-- SAVING Autocorrected Data In csv File --\n",
            "AutoCorrection Finished!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "class SpellChecker(object):\n",
        "\n",
        "    def read_corpus(self, filename):\n",
        "        with open(data_path, \"r\", encoding='utf-8-sig') as file:\n",
        "            lines = file.readlines() ## Read The File Line By Line\n",
        "            words = []\n",
        "            for line in lines:\n",
        "              words += re.findall(r'\\w+', line.lower()) ## Put Each Word In Its Lowercase.\n",
        "    \n",
        "        return words\n",
        "    \n",
        "    def read_corpus_lines(self, filename):\n",
        "        lines1 = []\n",
        "        lines2 = []\n",
        "        lines3 = []\n",
        "        lines4 = []\n",
        "        lines5 = []\n",
        "        lines6 = []\n",
        "        with open(filename, \"r\", encoding='utf-8-sig') as csvfile:\n",
        "            csvreader = csv.DictReader(csvfile)\n",
        "            for row in csvreader:\n",
        "                lines1.append(row[\"اسم الكاتب\"])\n",
        "                lines2.append(row[\"عنوان الكتاب\"])\n",
        "                lines3.append(row[\"نوع الكتاب\"])\n",
        "                lines4.append(row[\"رابط الكتاب\"])\n",
        "                lines5.append(row[\"معلومات عن الكتاب\"])\n",
        "                lines6.append(row[\"ملخص الكتاب\"])\n",
        "        return lines1, lines2, lines3, lines4,lines5,lines6\n",
        "    \n",
        "    def get_count(self, word_list):\n",
        "        word_count_dict = {}  ## Each Word Count\n",
        "        word_count_dict = Counter(word_list)\n",
        "        return word_count_dict\n",
        "    \n",
        "    def get_probs(self, word_count_dict):\n",
        "        ## 𝑃(𝑤ᵢ) = 𝐶(𝑤ᵢ) / M \n",
        "        m = sum(word_count_dict.values())\n",
        "        word_probs = {w: word_count_dict[w] / m for w in word_count_dict.keys()}\n",
        "        \n",
        "        return word_probs\n",
        "    \n",
        "    def _split(self, word):\n",
        "        return [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    \n",
        "    def _delete(self, word):\n",
        "        return [l + r[1:] for l,r in self._split(word) if r]\n",
        "    \n",
        "    def _swap(self, word):\n",
        "        return [l + r[1] + r[0] + r[2:] for l, r in self._split(word) if len(r)>1]\n",
        "    \n",
        "    def _replace(self, word):\n",
        "        letters = string.ascii_lowercase\n",
        "        return [l + c + r[1:] for l, r in self._split(word) if r for c in letters]\n",
        "    \n",
        "    def _insert(self, word):\n",
        "        letters = string.ascii_lowercase\n",
        "        return [l + c + r for l, r in self._split(word) for c in letters]\n",
        "       \n",
        "    def _edit1(self, word):  \n",
        "        return set(self._delete(word) + self._swap(word) + \n",
        "                   self._replace(word) + self._insert(word))\n",
        "    \n",
        "    def _edit2(self, word):\n",
        "      return set(e2 for e1 in self._edit1(word) for e2 in self._edit1(e1))\n",
        "    \n",
        "    def correct_spelling(self, word, vocabulary, word_probability):\n",
        "        if word in vocabulary:\n",
        "            #print(f\"\\n'{word}' is already correctly spelt\")\n",
        "            return \n",
        "        \n",
        "        suggestions = self._edit1(word) or self._edit2(word) or [word]\n",
        "        best_guesses = [w for w in suggestions if w in vocabulary]\n",
        "          \n",
        "        return [(w, word_probability[w]) for w in best_guesses]\n",
        "            \n",
        "    \n",
        "    def correct_word(self, word, corrections):\n",
        "        if corrections:\n",
        "            print('\\nSuggested Words:', corrections)\n",
        "            probs = np.array([c[1] for c in corrections])\n",
        "            ## Get The Index Of The Best Suggested Word (Higher Probability)\n",
        "            best_ix = np.argmax(probs) \n",
        "            correct = corrections[best_ix][0]\n",
        "            print(f\"\\n'{correct}' is Suggested for '{word}'\")\n",
        "            return correct        \n",
        "        \n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "\n",
        "### AutoCorrection\n",
        "def autocorrect_misspellings(lines, vocabs, word_prob):\n",
        "    correct_lines = []\n",
        "    for line in lines:\n",
        "        for word in line.split(' '):\n",
        "            corrections = spell_checker.correct_spelling(word, vocabs, word_prob)\n",
        "            correct     = spell_checker.correct_word(word, corrections)\n",
        "            if correct:\n",
        "                correct_lines.append(correct)\n",
        "            else:\n",
        "                correct_lines.append(word)\n",
        "\n",
        "    correct_lines = ' '.join(correct_lines)\n",
        "    \n",
        "    return correct_lines\n",
        "\n",
        "spell_checker = SpellChecker()\n",
        "print(\"Starting AutoCorrecting Misspelling....\\n\")\n",
        "base_words = spell_checker.read_corpus(dictionary_file)\n",
        "vocabs = set(base_words) ## Vocabulary (Unique Words)\n",
        "\n",
        "word_dict_counts = spell_checker.get_count(base_words)\n",
        "word_prob   = spell_checker.get_probs(word_dict_counts) \n",
        "\n",
        "authors, titles, categories, links,Summerztion,Info = spell_checker.read_corpus_lines(extracted_data_file)   \n",
        "authors = list(map(lambda orig_string: orig_string+'،،،،', authors))   \n",
        "titles = list(map(lambda orig_string: orig_string+'،،،،', titles))   \n",
        "categories = list(map(lambda orig_string: orig_string+'،،،،', categories)) \n",
        "Summerztion = list(map(lambda orig_string: orig_string+'،،،،', Summerztion))\n",
        "Info = list(map(lambda orig_string: orig_string+'،،،،', Info))\n",
        "\n",
        "correct_authors    = autocorrect_misspellings(authors, vocabs, word_prob)\n",
        "correct_titles     = autocorrect_misspellings(titles, vocabs, word_prob)\n",
        "correct_categories = autocorrect_misspellings(categories, vocabs, word_prob)\n",
        "correct_Summerztion = autocorrect_misspellings(Summerztion, vocabs, word_prob)\n",
        "correct_Info = autocorrect_misspellings(Info, vocabs, word_prob)\n",
        "\n",
        "\n",
        "with open(autocorrected_data_file, \"w\", encoding='utf-8-sig') as file:\n",
        "    print(\"-- SAVING Autocorrected Data In csv File --\")\n",
        "    wr = csv.writer(file)\n",
        "    wr.writerow(header)\n",
        "    \n",
        "    lines1 = correct_authors.split(delimiter)\n",
        "    lines2 = correct_titles.split(delimiter)\n",
        "    lines3 = correct_categories.split(delimiter)\n",
        "    \n",
        "    for (l1, l2, l3, l4) in zip(lines1, lines2, lines3, links):\n",
        "        s = u','.join([str(l1), str(l2), str(l3), str(l4)]) + u'\\n'\n",
        "        file.write(s)\n",
        "        \n",
        "print(\"AutoCorrection Finished!\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12E_nz_Bg2ri"
      },
      "outputs": [],
      "source": [
        "header = ['اسم الكاتب', 'عنوان الكتاب', 'نوع الكتاب', 'رابط الكتاب','معلومات عن الكتاب','ملخص الكتاب']\n",
        "extracted_data_file     = '/content/drive/MyDrive/S3_phase2/Scrapped_Data.csv'\n",
        "cleaned_data_file       = '/content/drive/MyDrive/S3_phase2/data_modified.csv'\n",
        "autocorrected_data_file = '/content/drive/MyDrive/S3_phase2/data_autocorrected.csv'\n",
        "dictionary_file         = './arabic-wordlist-1.6.txt'\n",
        "delimiter = '،،،،'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/S3_phase2/scrping_data.csv\"\n",
        "path = '/content/drive/MyDrive'\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "\n",
        "### Step 2) Data_preprocessing\n",
        "\n",
        "class preprocessing(object):\n",
        "    \n",
        "    def __init__(self, authors, titles, categories, links,Summerztion,Info):\n",
        "        \n",
        "        modified_authors    = self.Data_preprocessing(authors)\n",
        "        modified_titles     = self.Data_preprocessing(titles)\n",
        "        modified_categories = self.Data_preprocessing(categories)\n",
        "        modified_summerztion =self.Data_preprocessing(Summerztion)\n",
        "        modified_info =self.Data_preprocessing(Info)\n",
        "        ## Write Cleaned Scrapped Data Into New Text File\n",
        "        #save modified Data In csv File \n",
        "        with open(cleaned_data_file, \"w\", encoding='utf-8-sig') as file:\n",
        "            wr = csv.writer(file)\n",
        "            wr.writerow(header)\n",
        "            colum1 = modified_authors.split(delimiter)\n",
        "            colum2 = modified_titles.split(delimiter)\n",
        "            colum3 = modified_categories.split(delimiter)\n",
        "            colum4 = modified_summerztion.split(delimiter)\n",
        "            colum5 = modified_info.split(delimiter)\n",
        "            \n",
        "            for (l1, l2, l3, l4) in zip(colum1, colum2, colum3,colum4,colum5, links):\n",
        "                s = u','.join([str(l1), str(l2), str(l3), str(l4)]) + u'\\n'\n",
        "                file.write(s)\n",
        "                \n",
        "          \n",
        "      \n",
        "\n",
        "    def Data_preprocessing(self, text):\n",
        "        ## 1.1 Replace punctuations with a white space\n",
        "        remove_punctuations = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "        ## 1.2 Normalize\n",
        "        normalized = re.sub(\"گ\", \"ك\", remove_punctuations)\n",
        "        normalized = re.sub(\"ى\", \"ي\", normalized)\n",
        "        ## 2.1 Remove Non-Arabic Words\n",
        "        remove_nonarabic = re.sub(r'\\s*[A-Za-z]+\\b', ' ' , normalized)\n",
        "        ## 2.2 Remove Stop Words.\n",
        "        stop_words = set(stopwords.words('arabic'))\n",
        "        filtered_sentence = ' '.join([word for word in remove_nonarabic.split()if word not in stop_words]) \n",
        "        ## 3.0 Stemming\n",
        "        st = ISRIStemmer()\n",
        "        st.stem(remove_nonarabic)\n",
        "        \n",
        "        return remove_nonarabic\n",
        "    \n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "class SpellChecker(object):\n",
        "\n",
        "    def read_corpus(self, filename):\n",
        "        with open(data_path, \"r\", encoding='utf-8-sig') as file:\n",
        "            lines = file.readlines() ## Read The File Line By Line\n",
        "            words = []\n",
        "            for line in lines:\n",
        "              words += re.findall(r'\\w+', line.lower()) ## Put Each Word In Its Lowercase.\n",
        "    \n",
        "        return words\n",
        "    \n",
        "    def read_corpus_lines(self, filename):\n",
        "        lines1 = []\n",
        "        lines2 = []\n",
        "        lines3 = []\n",
        "        lines4 = []\n",
        "        lines5 = []\n",
        "        lines6 = []\n",
        "        with open(filename, \"r\", encoding='utf-8-sig') as csvfile:\n",
        "            csvreader = csv.DictReader(csvfile)\n",
        "            for row in csvreader:\n",
        "                lines1.append(row[\"اسم الكاتب\"])\n",
        "                lines2.append(row[\"عنوان الكتاب\"])\n",
        "                lines3.append(row[\"نوع الكتاب\"])\n",
        "                lines4.append(row[\"رابط الكتاب\"])\n",
        "                lines5.append(row[\"معلومات عن الكتاب\"])\n",
        "                lines6.append(row[\"ملخص الكتاب\"])\n",
        "        return lines1, lines2, lines3, lines4,lines5,lines6\n",
        "    \n",
        "    def get_count(self, word_list):\n",
        "        word_count_dict = {}  ## Each Word Count\n",
        "        word_count_dict = Counter(word_list)\n",
        "        return word_count_dict\n",
        "    \n",
        "    def get_probs(self, word_count_dict):\n",
        "        ## 𝑃(𝑤ᵢ) = 𝐶(𝑤ᵢ) / M \n",
        "        m = sum(word_count_dict.values())\n",
        "        word_probs = {w: word_count_dict[w] / m for w in word_count_dict.keys()}\n",
        "        \n",
        "        return word_probs\n",
        "    \n",
        "    def _split(self, word):\n",
        "        return [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    \n",
        "    def _delete(self, word):\n",
        "        return [l + r[1:] for l,r in self._split(word) if r]\n",
        "    \n",
        "    def _swap(self, word):\n",
        "        return [l + r[1] + r[0] + r[2:] for l, r in self._split(word) if len(r)>1]\n",
        "    \n",
        "    def _replace(self, word):\n",
        "        letters = string.ascii_lowercase\n",
        "        return [l + c + r[1:] for l, r in self._split(word) if r for c in letters]\n",
        "    \n",
        "    def _insert(self, word):\n",
        "        letters = string.ascii_lowercase\n",
        "        return [l + c + r for l, r in self._split(word) for c in letters]\n",
        "       \n",
        "    def _edit1(self, word):  \n",
        "        return set(self._delete(word) + self._swap(word) + \n",
        "                   self._replace(word) + self._insert(word))\n",
        "    \n",
        "    def _edit2(self, word):\n",
        "      return set(e2 for e1 in self._edit1(word) for e2 in self._edit1(e1))\n",
        "    \n",
        "    def correct_spelling(self, word, vocabulary, word_probability):\n",
        "        if word in vocabulary:\n",
        "            #print(f\"\\n'{word}' is already correctly spelt\")\n",
        "            return \n",
        "        \n",
        "        suggestions = self._edit1(word) or self._edit2(word) or [word]\n",
        "        best_guesses = [w for w in suggestions if w in vocabulary]\n",
        "          \n",
        "        return [(w, word_probability[w]) for w in best_guesses]\n",
        "            \n",
        "    \n",
        "    def correct_word(self, word, corrections):\n",
        "        if corrections:\n",
        "            print('\\nSuggested Words:', corrections)\n",
        "            probs = np.array([c[1] for c in corrections])\n",
        "            ## Get The Index Of The Best Suggested Word (Higher Probability)\n",
        "            best_ix = np.argmax(probs) \n",
        "            correct = corrections[best_ix][0]\n",
        "            print(f\"\\n'{correct}' is Suggested for '{word}'\")\n",
        "            return correct        \n",
        "        \n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "\n",
        "\n",
        "### preprocessing\n",
        "def split_file(filename):\n",
        "    data = pandas.read_csv(data_path)\n",
        "    \n",
        "    # converting columns data to list\n",
        "    authors = data['اسم الكاتب'].tolist()\n",
        "    titles = data['عنوان الكتاب'].tolist()\n",
        "    categories = data['نوع الكتاب'].tolist()\n",
        "    links = data['رابط الكتاب'].tolist()\n",
        "    Summerztion = data['ملخص الكتاب'].tolist()\n",
        "    Info = data['معلومات عن الكتاب'].tolist()\n",
        "\n",
        "\n",
        "    return authors, titles, categories, links,Summerztion,Info\n",
        "\n",
        "authors, titles, categories, links,Summerztion,Info = split_file(extracted_data_file)\n",
        "\n",
        "authors = list(map(lambda s: s+delimiter, authors))\n",
        "titles = list(map(lambda s: s+delimiter, titles))\n",
        "categories = list(map(lambda s: s+delimiter, categories))\n",
        "Summerztion =list(map(lambda s: s+delimiter,Summerztion))\n",
        "Info =list(map(lambda s: s+delimiter,Info))\n",
        "\n",
        "\n",
        "preprocessing(str(authors), str(titles), str(categories),str(Summerztion),str(Info), links)        \n",
        "\n",
        "### AutoCorrection\n",
        "def autocorrect_misspellings(lines, vocabs, word_prob):\n",
        "    correct_lines = []\n",
        "    for line in lines:\n",
        "        for word in line.split(' '):\n",
        "            corrections = spell_checker.correct_spelling(word, vocabs, word_prob)\n",
        "            correct     = spell_checker.correct_word(word, corrections)\n",
        "            if correct:\n",
        "                correct_lines.append(correct)\n",
        "            else:\n",
        "                correct_lines.append(word)\n",
        "\n",
        "    correct_lines = ' '.join(correct_lines)\n",
        "    \n",
        "    return correct_lines\n",
        "\n",
        "spell_checker = SpellChecker()\n",
        "print(\"Starting AutoCorrecting Misspelling....\\n\")\n",
        "base_words = spell_checker.read_corpus(dictionary_file)\n",
        "vocabs = set(base_words) ## Vocabulary (Unique Words)\n",
        "\n",
        "word_dict_counts = spell_checker.get_count(base_words)\n",
        "word_prob   = spell_checker.get_probs(word_dict_counts) \n",
        "\n",
        "authors, titles, categories, links,Summerztion,Info = spell_checker.read_corpus_lines(extracted_data_file)   \n",
        "authors = list(map(lambda orig_string: orig_string+'،،،،', authors))   \n",
        "titles = list(map(lambda orig_string: orig_string+'،،،،', titles))   \n",
        "categories = list(map(lambda orig_string: orig_string+'،،،،', categories)) \n",
        "Summerztion = list(map(lambda orig_string: orig_string+'،،،،', Summerztion))\n",
        "Info = list(map(lambda orig_string: orig_string+'،،،،', Info))\n",
        "\n",
        "correct_authors    = autocorrect_misspellings(authors, vocabs, word_prob)\n",
        "correct_titles     = autocorrect_misspellings(titles, vocabs, word_prob)\n",
        "correct_categories = autocorrect_misspellings(categories, vocabs, word_prob)\n",
        "correct_Summerztion = autocorrect_misspellings(Summerztion, vocabs, word_prob)\n",
        "correct_Info = autocorrect_misspellings(Info, vocabs, word_prob)\n",
        "\n",
        "\n",
        "with open(autocorrected_data_file, \"w\", encoding='utf-8-sig') as file:\n",
        "    print(\"-- SAVING Autocorrected Data In csv File --\")\n",
        "    wr = csv.writer(file)\n",
        "    wr.writerow(header)\n",
        "    \n",
        "    lines1 = correct_authors.split(delimiter)\n",
        "    lines2 = correct_titles.split(delimiter)\n",
        "    lines3 = correct_categories.split(delimiter)\n",
        "    \n",
        "    for (l1, l2, l3, l4) in zip(lines1, lines2, lines3, links):\n",
        "        s = u','.join([str(l1), str(l2), str(l3), str(l4)]) + u'\\n'\n",
        "        file.write(s)\n",
        "        \n",
        "print(\"AutoCorrection Finished!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1MW1gzAFgcX",
        "outputId": "a2d6a97f-b9fa-4bbf-c6ce-0ff4c145a9f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    اسم الكاتب  ...                                        ملخص الكتاب\n",
            "0  كتاب التاريخ الصغير ج 2 PDF  ...  \\n\\n\\n\\r\\n     (adsbygoogle = window.adsbygoog...\n",
            "\n",
            "[1 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "filePath=\"/content/drive/MyDrive/S3_phase2/Scrapped_Data.csv\"\n",
        "dataframe = pandas.read_csv(filePath, header=0)\n",
        "dataframe.drop_duplicates(inplace=True)\n",
        "data = dataframe\n",
        "print(dataframe.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW8llx7iFtT4",
        "outputId": "071a733e-27b5-4a41-98da-aaf36803cad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "print(type(data['اسم الكاتب']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ_-4JDWFyl0"
      },
      "outputs": [],
      "source": [
        "train_size = int(len(data)*0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pM29yyuF34G"
      },
      "outputs": [],
      "source": [
        "name = data['اسم الكاتب']\n",
        "category = data['نوع الكتاب']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQbwkfymF74s"
      },
      "outputs": [],
      "source": [
        "train_names = name[:train_size]\n",
        "train_categories = category[:train_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtBeDlUoF_VS"
      },
      "outputs": [],
      "source": [
        "test_names = name[train_size:]\n",
        "test_categories = category[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gPZlotOGBOJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=None, lower=False)\n",
        "tokenizer.fit_on_texts(test_names) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJLUzrXIGc8Y"
      },
      "outputs": [],
      "source": [
        "x_train = tokenizer.texts_to_matrix(train_names, mode='tfidf')  # tfidf  ---> every word have weight\n",
        "x_test = tokenizer.texts_to_matrix(test_names, mode='tfidf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaUB_40wGmUe"
      },
      "outputs": [],
      "source": [
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muZ7u4kTGqrm"
      },
      "outputs": [],
      "source": [
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6hdjK8jGsIV"
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(category)\n",
        "tagst = encoder.fit_transform(category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p2qOi5FGwRZ"
      },
      "outputs": [],
      "source": [
        "num_classes = int((len(set(tagst))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS5XqoBnG0Ip"
      },
      "outputs": [],
      "source": [
        "y_train = encoder.fit_transform(train_categories)\n",
        "y_test = encoder.fit_transform(test_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqa0DRbOG4lt"
      },
      "outputs": [],
      "source": [
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et664D_CG7v5"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDaoQF1EG9K-"
      },
      "outputs": [],
      "source": [
        "max_words = vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quhnumiYG_x0"
      },
      "outputs": [],
      "source": [
        "def f1_metric(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "    return f1_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_q21XuNHEos"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGLfrUyVHHxG"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['categorical_accuracy', 'Recall', 'Precision',\n",
        "                       f1_metric, 'TruePositives', 'TrueNegatives', 'FalsePositives',\n",
        "                       'FalseNegatives'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA5XWbHLHLAo"
      },
      "outputs": [],
      "source": [
        "batch_size = 5\n",
        "epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "-_jwq8sbHNY1",
        "outputId": "139d9655-a558-4ed2-9135-4af5424e1403"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-22cb9b19d5ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     validation_split=0.1)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_model.h1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0;34m\"`validation_split={validation_split}`. Either provide more data, or a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \"different value for the `validation_split` argument.\" .format(\n\u001b[0;32m-> 1505\u001b[0;31m             batch_dim=batch_dim, validation_split=validation_split))\n\u001b[0m\u001b[1;32m   1506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.1`. Either provide more data, or a different value for the `validation_split` argument."
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "\n",
        "model.save('my_model.h1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPRpxdyHHPt6"
      },
      "outputs": [],
      "source": [
        "Evaluation_valus = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Loss\", 'categorical_accuracy', 'Recall', 'Precision',\n",
        "      'f1_metric', 'TruePositives', 'TrueNegatives', 'FalsePositives',\n",
        "      'FalseNegatives')\n",
        "\n",
        "print(Evaluation_valus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0-nYiARHSKW"
      },
      "outputs": [],
      "source": [
        "def unique(tags):\n",
        "    unique_list = []\n",
        "    un = []\n",
        "    for x in tags:\n",
        "        if x not in unique_list:\n",
        "            unique_list.append(x)\n",
        "\n",
        "    unique_list.sort()\n",
        "    return unique_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWRs--KsHUom"
      },
      "outputs": [],
      "source": [
        "un_list=[]\n",
        "print(\"the unique values from 1st list is\")\n",
        "un_list =unique(category)\n",
        "print(un_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rygUwgHMHXEQ"
      },
      "outputs": [],
      "source": [
        "for x in data[\"اسم الكاتب\"][0:20]:\n",
        "    tokens = tokenizer.texts_to_matrix([x], mode='tfidf')\n",
        "    predict_x=model.predict(tokens) \n",
        "    classes_x=np.argmax(predict_x,axis=1)\n",
        "    print(predict_x,\"= \\t\",classes_x,\"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYalBPyhtMix"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Selected_3proect_(2).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}